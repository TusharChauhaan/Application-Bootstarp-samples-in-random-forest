{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAHap1Z3FZC-",
    "papermill": {
     "duration": 0.015934,
     "end_time": "2021-07-17T15:54:51.310248",
     "exception": false,
     "start_time": "2021-07-17T15:54:51.294314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bootstrap Assignmnet\n",
    "\n",
    "### There will be some functions that start with the word \"grader\" ex: grader_sampples(), grader_30().. etc, we should not change those function definition. Every Grader function has to return True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:51.352122Z",
     "iopub.status.busy": "2021-07-17T15:54:51.349561Z",
     "iopub.status.idle": "2021-07-17T15:54:52.451382Z",
     "shell.execute_reply": "2021-07-17T15:54:52.450864Z",
     "shell.execute_reply.started": "2021-07-17T15:11:30.865446Z"
    },
    "id": "m6ag91ijrQOs",
    "papermill": {
     "duration": 1.126167,
     "end_time": "2021-07-17T15:54:52.451528",
     "exception": false,
     "start_time": "2021-07-17T15:54:51.325361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape  (506, 13)\n",
      "y.shape  (506,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # importing numpy for numerical computation\n",
    "from sklearn.datasets import load_boston # here we are using sklearn's boston dataset\n",
    "from sklearn.metrics import mean_squared_error # importing mean_squared_error metric\n",
    "\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "boston = load_boston()\n",
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable\n",
    "\n",
    "print(\"x.shape \", x.shape)\n",
    "print(\"y.shape \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2fHTdS_zpgG",
    "papermill": {
     "duration": 0.014943,
     "end_time": "2021-07-17T15:54:52.522232",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.507289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Task 1\n",
    "\n",
    "*  <font color='blue'><b>Creating samples</b></font><br>\n",
    "    <b> Randomly create 30 samples from the whole boston data points</b>\n",
    "    *  Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points\n",
    "    \n",
    "     For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly , consider we have selected [4, 5, 7, 8, 9, 3] now we will replicate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]\n",
    "* <font color='blue'><b> Create 30 samples </b></font>\n",
    "    *  Note that as a part of the Bagging when we are taking the random samples <b>make sure each of the sample will have different set of columns</b><br>\n",
    "Ex: Assume we have 10 columns[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10] for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample  [7, 9, 1, 4, 5, 6, 2] and so on...\n",
    "Make sure each sample will have atleast 3 feautres/columns/attributes\n",
    "\n",
    "### Step - 1 - Creating samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSVaWG1F4uCZ",
    "papermill": {
     "duration": 0.015075,
     "end_time": "2021-07-17T15:54:52.552981",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.537906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font color='Orange'><b>Algorithm</b></font>\n",
    "\n",
    "![alt text](https://i.imgur.com/BTVYXQ1.jpg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_oWoN97BhDY",
    "papermill": {
     "duration": 0.014934,
     "end_time": "2021-07-17T15:54:52.583292",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.568358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:52.621391Z",
     "iopub.status.busy": "2021-07-17T15:54:52.620686Z",
     "iopub.status.idle": "2021-07-17T15:54:52.623533Z",
     "shell.execute_reply": "2021-07-17T15:54:52.623090Z",
     "shell.execute_reply.started": "2021-07-17T15:11:32.095354Z"
    },
    "papermill": {
     "duration": 0.025187,
     "end_time": "2021-07-17T15:54:52.623653",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.598466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generating_samples(input_data, target_data): \n",
    "  \n",
    "  # Here we are finding the random 303 row indices without replacement as shown in the picture above\n",
    "  selected_rows = np.random.choice(len(input_data), 303, replace=False)\n",
    "  \n",
    "  #selecting 203 more row indices from the selected rows.\n",
    "  get_203_from_selected_rows = np.random.choice(selected_rows, 203, replace=False)\n",
    "  \n",
    "  # Now get 3 to 13 random column indices from input_data\n",
    "  random_selected_columns = random.randint(3, 13)\n",
    "  columns_selected = np.array(random.sample(range(0, 13), random_selected_columns ))\n",
    "  #Taken from:: https://stackoverflow.com/questions/22927181/selecting-specific-rows-and-columns-from-numpy-array\n",
    "  sample_data = input_data[selected_rows[:, None], columns_selected]\n",
    "  \n",
    "  target_of_sample_data = target_data[selected_rows]\n",
    "  \n",
    "  # Now Replication of Data for 203 data points out of 303 selected points\n",
    "  \n",
    "  replicated_203_sample_data_points = input_data[get_203_from_selected_rows[:, None], columns_selected]\n",
    "#   print(get_203_from_selected_rows)\n",
    "#   print(type(get_203_from_selected_rows))\n",
    "#   print(\"this is printed \",get_203_from_selected_rows[:, None])\n",
    "#   print(\"this is end\")\n",
    "#   print(columns_selected)\n",
    "  target_203_replicated_sample_data = target_data[get_203_from_selected_rows]\n",
    "  \n",
    "  # Concatenating data\n",
    "  \n",
    "  final_sample_data = np.vstack((sample_data, replicated_203_sample_data_points ))\n",
    "  \n",
    "  final_target_data = np.vstack((target_of_sample_data.reshape(-1, 1), target_203_replicated_sample_data.reshape(-1, 1) ))\n",
    "  \n",
    "  return final_sample_data, final_target_data, selected_rows, columns_selected\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MivEQFlm7iOg",
    "papermill": {
     "duration": 0.014791,
     "end_time": "2021-07-17T15:54:52.653781",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.638990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font color='cyan'> <b> Grader function - 1 </b> </fongt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:52.693193Z",
     "iopub.status.busy": "2021-07-17T15:54:52.692240Z",
     "iopub.status.idle": "2021-07-17T15:54:52.771720Z",
     "shell.execute_reply": "2021-07-17T15:54:52.771290Z",
     "shell.execute_reply.started": "2021-07-17T15:11:32.108373Z"
    },
    "id": "AVvuhNzm7uld",
    "papermill": {
     "duration": 0.102612,
     "end_time": "2021-07-17T15:54:52.771834",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.669222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 11)\n",
      "(506, 1)\n",
      "(303,)\n",
      "(11,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_samples(a,b,c,d):\n",
    "    length = (len(a)==506  and len(b)==506)\n",
    "    sampled = (len(a)-len(set([str(i) for i in a]))==203)\n",
    "    rows_length = (len(c)==303)\n",
    "    column_length= (len(d)>=3)\n",
    "    assert(length and sampled and rows_length and column_length)\n",
    "    return True\n",
    "  \n",
    "a,b,c,d = generating_samples(x, y)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(d.shape)\n",
    "grader_samples(a,b,c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4LSsmn4Jn2_",
    "papermill": {
     "duration": 0.016356,
     "end_time": "2021-07-17T15:54:52.804205",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.787849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating 30 samples\n",
    "\n",
    "![alt text](https://i.imgur.com/p8eZaWL.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:52.842431Z",
     "iopub.status.busy": "2021-07-17T15:54:52.841633Z",
     "iopub.status.idle": "2021-07-17T15:54:52.849067Z",
     "shell.execute_reply": "2021-07-17T15:54:52.849422Z",
     "shell.execute_reply.started": "2021-07-17T15:11:32.232918Z"
    },
    "id": "XXlKWjCcBvTk",
    "papermill": {
     "duration": 0.02892,
     "end_time": "2021-07-17T15:54:52.849556",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.820636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting 30 sample data with their column and row details\n",
    "list_input_data =[]\n",
    "list_output_data =[]\n",
    "list_selected_row= []\n",
    "list_selected_columns=[]\n",
    "\n",
    "for i in range (0, 30):\n",
    "  a, b, c, d = generating_samples(x, y)\n",
    "  list_input_data.append(a)\n",
    "  list_output_data.append(b)\n",
    "  list_selected_row.append(c)\n",
    "  list_selected_columns.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXUz9VFiMQkh",
    "papermill": {
     "duration": 0.016752,
     "end_time": "2021-07-17T15:54:52.882522",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.865770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font color='cyan'> <b>Grader function - 2 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:52.920230Z",
     "iopub.status.busy": "2021-07-17T15:54:52.919531Z",
     "iopub.status.idle": "2021-07-17T15:54:52.923012Z",
     "shell.execute_reply": "2021-07-17T15:54:52.922577Z",
     "shell.execute_reply.started": "2021-07-17T15:11:32.245796Z"
    },
    "id": "hCvIq8NuMWOC",
    "papermill": {
     "duration": 0.024303,
     "end_time": "2021-07-17T15:54:52.923111",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.898808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_30(a):\n",
    "    assert(len(a)==30 and len(a[0])==506)\n",
    "    return True\n",
    "grader_30(list_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whaHCPB0O8qF",
    "papermill": {
     "duration": 0.015894,
     "end_time": "2021-07-17T15:54:52.954947",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.939053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step - 2 of Task-1\n",
    "\n",
    "\n",
    "## Building High Variance Models on each of the sample and finding train MSE value\n",
    "\n",
    "\n",
    "*  Build a regression trees on each of 30 samples.\n",
    "*  Computed the predicted values of each data point(506 data points) in our corpus.\n",
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$\n",
    "*  Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$\n",
    "\n",
    "## Flowchart for Building regression trees\n",
    "\n",
    "![alt text](https://i.imgur.com/pcXfSmp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:52.992355Z",
     "iopub.status.busy": "2021-07-17T15:54:52.991877Z",
     "iopub.status.idle": "2021-07-17T15:54:53.058291Z",
     "shell.execute_reply": "2021-07-17T15:54:53.057833Z",
     "shell.execute_reply.started": "2021-07-17T15:11:32.256432Z"
    },
    "id": "YWQp6tRwMthq",
    "papermill": {
     "duration": 0.087367,
     "end_time": "2021-07-17T15:54:53.058402",
     "exception": false,
     "start_time": "2021-07-17T15:54:52.971035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_model_list = []\n",
    "for i in range(0, 30):\n",
    "  Dtree_i = DecisionTreeRegressor()\n",
    "  Dtree_i.fit(list_input_data[i], list_output_data[i])\n",
    "  all_model_list.append(Dtree_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21j8BKfAQ1U8",
    "papermill": {
     "duration": 0.016779,
     "end_time": "2021-07-17T15:54:53.091926",
     "exception": false,
     "start_time": "2021-07-17T15:54:53.075147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Flowchart for calculating MSE\n",
    "\n",
    "![alt text](https://i.imgur.com/sPEE618.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e-UamlHRjPy",
    "papermill": {
     "duration": 0.017134,
     "end_time": "2021-07-17T15:54:53.126068",
     "exception": false,
     "start_time": "2021-07-17T15:54:53.108934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After getting predicted_y for each data point, we can use sklearns mean_squared_error to calculate the MSE between predicted_y and actual_y.\n",
    "\n",
    "# Calculating MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:53.166194Z",
     "iopub.status.busy": "2021-07-17T15:54:53.165598Z",
     "iopub.status.idle": "2021-07-17T15:54:53.182185Z",
     "shell.execute_reply": "2021-07-17T15:54:53.181598Z",
     "shell.execute_reply.started": "2021-07-17T15:11:32.347527Z"
    },
    "id": "qWhcvMRWRA9b",
    "papermill": {
     "duration": 0.039296,
     "end_time": "2021-07-17T15:54:53.182302",
     "exception": false,
     "start_time": "2021-07-17T15:54:53.143006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of predicted median  (506,)\n",
      "MSE :  0.0631836023276241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import median\n",
    "\n",
    "array_of_Y = []\n",
    "\n",
    "for i in range(0, 30):\n",
    "  data_point_i = x[:, list_selected_columns[i]]\n",
    "  target_Y = all_model_list[i].predict(data_point_i)\n",
    "  array_of_Y.append(target_Y)\n",
    "  \n",
    "\n",
    "predicted_array_of_target_y = np.array(array_of_Y)\n",
    "predicted_array_of_target_y = predicted_array_of_target_y.transpose()\n",
    "\n",
    "# Now to calculate MSE, first calculate the Median of Predicted Y\n",
    "# passing axis=1 will make sure the medians are computed along axis=1\n",
    "median_predicted_y = np.median(predicted_array_of_target_y, axis=1)\n",
    "\n",
    "print(\"The shape of predicted median \",median_predicted_y.shape)\n",
    "print(\"MSE : \", mean_squared_error(y, median_predicted_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuclPDMnSz8F",
    "papermill": {
     "duration": 0.016483,
     "end_time": "2021-07-17T15:54:53.215944",
     "exception": false,
     "start_time": "2021-07-17T15:54:53.199461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step - 3 of Task-1\n",
    "\n",
    "### Calculating the OOB score\n",
    "\n",
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.\n",
    "*  Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.\n",
    "\n",
    "\n",
    "### Given a single query point predict the price of house\n",
    "\n",
    "Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
    "Predict the house price for this point as mentioned in the step 2 of Task 1.\n",
    "\n",
    "## Flowchart for calculating OOB score\n",
    "\n",
    "![alt text](https://i.imgur.com/95S5Mtm.png)\n",
    "\n",
    "## Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:53.259918Z",
     "iopub.status.busy": "2021-07-17T15:54:53.258693Z",
     "iopub.status.idle": "2021-07-17T15:54:53.852104Z",
     "shell.execute_reply": "2021-07-17T15:54:53.851569Z",
     "shell.execute_reply.started": "2021-07-17T15:11:32.370496Z"
    },
    "id": "Fog_6DNdS-h_",
    "papermill": {
     "duration": 0.619253,
     "end_time": "2021-07-17T15:54:53.852219",
     "exception": false,
     "start_time": "2021-07-17T15:54:53.232966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_oob_score is  17.738450576416326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_predicted_oob_median_list = []\n",
    "\n",
    "for i in range(0, 506):\n",
    "  indices_for_oob_models = []\n",
    "  \n",
    "  # For each of i-th row I shall build a list, of sample size 30\n",
    "  # ONLY condition being that this i-th row should not be part of the list_selected_row[i-th]\n",
    "  # e.g. say for i = 469 and index_oob in below loop is 10 then \n",
    "  # list_selected_row[10] (which is an array of row-numbers) should not contain the 469-th row\n",
    "  for index_oob in range(0, 30):\n",
    "    if i not in list_selected_row[index_oob]:\n",
    "      indices_for_oob_models.append(index_oob)\n",
    "      \n",
    "  y_predicted_oob_list = []\n",
    "  \n",
    "  for oob_Dtree_index in indices_for_oob_models:\n",
    "    model_oob = all_model_list[oob_Dtree_index]\n",
    "    \n",
    "    row_oob = x[i]\n",
    "    # print('oob_Dtree_index ', oob_Dtree_index)\n",
    "    \n",
    "    # Now extract ONLY those specific columns/featues that were selected during the bootstrapping\n",
    "    x_oob_data_point = [row_oob[columns] for columns in list_selected_columns[oob_Dtree_index] ]\n",
    "    # print('np.array(x_oob_data_point) ', np.array(x_oob_data_point))\n",
    "    x_oob_data_point = np.array(x_oob_data_point).reshape(1, -1)\n",
    "    \n",
    "    y_predicted_oob_data_point = model_oob.predict(x_oob_data_point)\n",
    "    y_predicted_oob_list.append(y_predicted_oob_data_point)\n",
    "    # \n",
    "  y_predicted_oob_list = np.array(y_predicted_oob_list)\n",
    "  \n",
    "  y_predicted_median = np.median(y_predicted_oob_list)\n",
    "  y_predicted_oob_median_list.append(y_predicted_median)\n",
    "  \n",
    "\n",
    "def calculate_oob_score(num_rows):\n",
    "  oob_score = 0\n",
    "  for i in range(0, num_rows):\n",
    "    oob_score += ((y[i] - y_predicted_oob_median_list[i] ) ** 2)\n",
    "  final_oob_score = oob_score/506\n",
    "  return final_oob_score\n",
    "\n",
    "print(\"final_oob_score is \", calculate_oob_score(506))   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016999,
     "end_time": "2021-07-17T15:54:53.886137",
     "exception": false,
     "start_time": "2021-07-17T15:54:53.869138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Further notes on above OOB calculation\n",
    "\n",
    "The key point is that the OOB sample rows were passed through every Decition Treee that did not contain those specific OOB sample rows during the bootstrapping of training data.\n",
    "\n",
    "OOB error is simply the error on samples that were not seen during training. \n",
    "\n",
    "OOB Scoring is very useful when I dont have a large dataset and thereby if I split that dataset into training and validation set - will result in loss of useful data that otherwise could have been used for training the models. Hence in this case, we decide to extract some of the training data as the validation set by using only those data-points that were not used for training a particular sample-set.\n",
    "\n",
    "---\n",
    "\n",
    "# Task 2\n",
    "\n",
    "* Computing CI of OOB Score and Train MSE\n",
    "* Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score\n",
    "* After this we will have 35 Train MSE values and 35 OOB scores\n",
    "* using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score\n",
    "* we need to report CI of MSE and CI of OOB Score\n",
    "* Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:53.933075Z",
     "iopub.status.busy": "2021-07-17T15:54:53.932140Z",
     "iopub.status.idle": "2021-07-17T15:54:54.545398Z",
     "shell.execute_reply": "2021-07-17T15:54:54.544942Z",
     "shell.execute_reply.started": "2021-07-17T15:11:33.061061Z"
    },
    "papermill": {
     "duration": 0.642927,
     "end_time": "2021-07-17T15:54:54.545519",
     "exception": false,
     "start_time": "2021-07-17T15:54:53.902592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.013466976528570733, 12.777552353616024)\n"
     ]
    }
   ],
   "source": [
    "# Function to build the entire bootstrapping steps that we did above and\n",
    "# Reurning from the function the MSE and oob score\n",
    "def bootstrapping_and_oob(x, y):\n",
    "\n",
    "  # Use generating_samples function to create 30 samples \n",
    "  # store these created samples in a list\n",
    "  list_input_data =[]\n",
    "  list_output_data =[]\n",
    "  list_selected_row= []\n",
    "  list_selected_columns=[]\n",
    "  \n",
    "  for i in range (0, 30):\n",
    "    a, b, c, d = generating_samples(x, y)\n",
    "    list_input_data.append(a)\n",
    "    list_output_data.append(b)\n",
    "    list_selected_row.append(c)\n",
    "    list_selected_columns.append(d)\n",
    "  \n",
    "  # building regression trees\n",
    "  all_model_list = []\n",
    "  for i in range(0, 30):\n",
    "    Dtree_i = DecisionTreeRegressor(max_depth=None)\n",
    "    Dtree_i.fit(list_input_data[i], list_output_data[i])\n",
    "    all_model_list.append(Dtree_i)\n",
    "  \n",
    "  # calculating MSE\n",
    "  array_of_Y = []\n",
    "\n",
    "  for i in range(0, 30):\n",
    "    data_point_i = x[:, list_selected_columns[i]]\n",
    "    target_Y = all_model_list[i].predict(data_point_i)\n",
    "    array_of_Y.append(target_Y)\n",
    "    \n",
    "    \n",
    "  predicted_array_of_target_y = np.array(array_of_Y)\n",
    "  predicted_array_of_target_y = predicted_array_of_target_y.transpose()\n",
    "\n",
    "  # print(predicted_array_of_target_y.shape)\n",
    "\n",
    "  # Now to calculate MSE, first calculate the Median of Predicted Y\n",
    "  # passing axis=1 will make sure the medians are computed along axis=1\n",
    "  median_predicted_y = np.median(predicted_array_of_target_y, axis=1)\n",
    "  \n",
    "  # And now the final MSE\n",
    "  MSE = mean_squared_error(y, median_predicted_y )\n",
    "  \n",
    "  # Calculating OOB Score\n",
    "  y_predicted_oob_median_list = []\n",
    "\n",
    "  for i in range(0, 506):\n",
    "    indices_for_oob_models = []\n",
    "    \n",
    "    # For each of i-th row I shall build a list of sample size 30\n",
    "    # ONLY condition being that this ith row should not be part of\n",
    "    # the list_selected_row\n",
    "    for index_oob in range(0, 30):\n",
    "      if i not in list_selected_row[index_oob]:\n",
    "        indices_for_oob_models.append(index_oob)\n",
    "        \n",
    "    y_predicted_oob_list = []\n",
    "    \n",
    "    for oob_Dtree_index in indices_for_oob_models:\n",
    "      model_oob = all_model_list[oob_Dtree_index]\n",
    "      \n",
    "      row_oob = x[i]\n",
    "      # print('oob_Dtree_index ', oob_Dtree_index)\n",
    "      \n",
    "      x_oob_data_point = [row_oob[col] for col in list_selected_columns[oob_Dtree_index] ]\n",
    "      # print('np.array(x_oob_data_point) ', np.array(x_oob_data_point))\n",
    "      x_oob_data_point = np.array(x_oob_data_point).reshape(1, -1)\n",
    "      \n",
    "      y_predicted_oob_data_point = model_oob.predict(x_oob_data_point)\n",
    "      y_predicted_oob_list.append(y_predicted_oob_data_point)\n",
    "      # \n",
    "    y_predicted_oob_list = np.array(y_predicted_oob_list)\n",
    "    \n",
    "    y_predicted_median = np.median(y_predicted_oob_list)\n",
    "    y_predicted_oob_median_list.append(y_predicted_median)\n",
    "    \n",
    "\n",
    "  oob_score = 0\n",
    "\n",
    "  for i in range(0, 506):\n",
    "    # oob_score = (oob_score + (y[i] - y_predicted_oob_median_list[i] ) ** 2)\n",
    "    # 13.828377285079045\n",
    "    oob_score += (y[i] - y_predicted_oob_median_list[i] ) ** 2\n",
    "\n",
    "  final_oob_score = oob_score/506\n",
    "  \n",
    "  return MSE, final_oob_score\n",
    "\n",
    "print(bootstrapping_and_oob(x, y))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:54:54.588020Z",
     "iopub.status.busy": "2021-07-17T15:54:54.587455Z",
     "iopub.status.idle": "2021-07-17T15:55:16.170156Z",
     "shell.execute_reply": "2021-07-17T15:55:16.169627Z",
     "shell.execute_reply.started": "2021-07-17T15:11:33.803497Z"
    },
    "papermill": {
     "duration": 21.605832,
     "end_time": "2021-07-17T15:55:16.170273",
     "exception": false,
     "start_time": "2021-07-17T15:54:54.564441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence_interval_mse_35  (0.07434321719602137, 0.182678895068009)\n",
      "confidence_interval_oob_score_35  (13.229087513047887, 14.209067839788748)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable\n",
    "\n",
    "mse_boston_35_times_arr = []\n",
    "oob_score_boston_35_times_arr = []\n",
    "\n",
    "# Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score\n",
    "for i in range(0, 35):\n",
    "  mse, oob_score = bootstrapping_and_oob(x, y)\n",
    "  mse_boston_35_times_arr.append(mse)\n",
    "  oob_score_boston_35_times_arr.append(oob_score)\n",
    "\n",
    "\n",
    "mse_boston_35_times_arr = np.array(mse_boston_35_times_arr)\n",
    "oob_score_boston_35_times_arr = np.array(oob_score_boston_35_times_arr)\n",
    "\n",
    "confidence_level = 0.95\n",
    "degrees_of_freedom = 34 # sample.size - 1\n",
    "\n",
    "mean_of_sample_mse_35 = np.mean(mse_boston_35_times_arr)\n",
    "standard_error_mse = scipy.stats.sem(mse_boston_35_times_arr)\n",
    "\n",
    "\n",
    "# Per document - https://www.kite.com/python/answers/how-to-compute-the-confidence-interval-of-a-sample-statistic-in-python\n",
    "# confidence_interval = scipy.stats.t.interval(confidence_level, degrees_freedom, sample_mean, sample_standard_error)\n",
    "confidence_interval_mse_35 = scipy.stats.t.interval(confidence_level, degrees_of_freedom, mean_of_sample_mse_35, standard_error_mse )\n",
    "print(\"confidence_interval_mse_35 \", confidence_interval_mse_35)\n",
    "\n",
    "# Now calculate confidence inter for oob score\n",
    "mean_of_sample_oob_score_35 = np.mean(oob_score_boston_35_times_arr)\n",
    "standard_error_of_sample_oob_score_35 = scipy.stats.sem(oob_score_boston_35_times_arr)\n",
    "\n",
    "confidence_interval_oob_score_35 = scipy.stats.t.interval(confidence_level, degrees_of_freedom, mean_of_sample_oob_score_35, standard_error_of_sample_oob_score_35 )\n",
    "print(\"confidence_interval_oob_score_35 \", confidence_interval_oob_score_35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019728,
     "end_time": "2021-07-17T15:55:16.209957",
     "exception": false,
     "start_time": "2021-07-17T15:55:16.190229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Observation / Interpretation of above Confidence Interval\n",
    "\n",
    "By definition we know the interpretation of a 95% confidence interval for the population mean as  -  If repeated random samples were taken and the 95% confidence interval was computed for each sample, 95% of the intervals would contain the population mean.\n",
    "\n",
    "So in this case\n",
    "\n",
    "- MSE - There is a 95% chance that the confidence interval of (0.05732086175441538, 0.11667646077442519) contains the true population mean of MSE.\n",
    "- OOB Score - There is a 95% chance that the confidence interval of (13.274222499705303, 14.427942855729313) contains the true population mean of OOB Score.\n",
    "\n",
    "# Task 3 (send query point \"xq\" to 30 models)\n",
    "\n",
    "## We created 30 models by using 30 samples in TASK-1. Here, we need send query point \"xq\" to 30 models and perform the regression on the output generated by 30 models\n",
    "\n",
    "## Flowchart for Task 3\n",
    "\n",
    "![alt text](https://i.imgur.com/Y5cNhQk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:55:16.253738Z",
     "iopub.status.busy": "2021-07-17T15:55:16.253185Z",
     "iopub.status.idle": "2021-07-17T15:55:16.259294Z",
     "shell.execute_reply": "2021-07-17T15:55:16.259706Z",
     "shell.execute_reply.started": "2021-07-17T15:11:59.215457Z"
    },
    "id": "i_pUlSD-VYD1",
    "papermill": {
     "duration": 0.031636,
     "end_time": "2021-07-17T15:55:16.259834",
     "exception": false,
     "start_time": "2021-07-17T15:55:16.228198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_y_given_x_bootstrap(x_query):\n",
    "  y_predicted_array_30_sample = []\n",
    "  \n",
    "  for i in range(0, 30):\n",
    "    Dtree_i = all_model_list[i]\n",
    "    # Extract x for ith data point with specific number of featues from list_selected_columns\n",
    "    x_data_point_i = [x_query[column] for column in list_selected_columns[i]]\n",
    "    x_data_point_i = np.array(x_data_point_i).reshape(1, -1)\n",
    "    y_predicted_i = Dtree_i.predict(x_data_point_i)\n",
    "    y_predicted_array_30_sample.append(y_predicted_i)\n",
    "    \n",
    "  y_predicted_array_30_sample = np.array(y_predicted_array_30_sample)\n",
    "  y_predicted_median = np.median(y_predicted_array_30_sample)\n",
    "  return y_predicted_median\n",
    "\n",
    "\n",
    "xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60]\n",
    "y_predicted_for_xq = predict_y_given_x_bootstrap(xq)\n",
    "y_predicted_for_xq    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.516579,
   "end_time": "2021-07-17T15:55:16.885664",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-17T15:54:44.369085",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
